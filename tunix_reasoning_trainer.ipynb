{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunix Reasoning Model Trainer\n",
    "# Training Gemma2-2B with GRPO for Transparent Reasoning\n",
    "\n",
    "**Google Tunix Hackathon Submission**\n",
    "\n",
    "This notebook trains Gemma2 2B to produce step-by-step reasoning traces using:\n",
    "- GRPO (Group Relative Policy Optimization) with Tunix\n",
    "- Custom reward functions for reasoning quality\n",
    "- Format enforcement for structured output\n",
    "\n",
    "**Hardware**: TPU v3-8 (8+ hour session recommended)\n",
    "\n",
    "**Output Format**: `<reasoning>...</reasoning><answer>...</answer>`\n",
    "\n",
    "**Reference**: Based on the official Tunix GRPO demo pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tunix and dependencies\n",
    "!pip install -q git+https://github.com/google/tunix.git\n",
    "!pip install -q kagglehub\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "# Core imports\n",
    "import json\n",
    "import re\n",
    "import functools\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# JAX and numerical computing\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# Keras and model loading\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Tunix GRPO\n",
    "from tunix.grpo import grpo\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Installation complete\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Gemma2-2B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Gemma2 2B model\n",
    "# Note: You need to accept the Gemma license on Kaggle first\n",
    "\n",
    "model_id = \"gemma2_instruct_2b_en\"\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
    "\n",
    "print(f\"Model loaded: {model_id}\")\n",
    "print(f\"Model parameters: {gemma_lm.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    \"\"\"Configuration for GRPO training.\"\"\"\n",
    "    \n",
    "    # GRPO parameters\n",
    "    num_generations: int = 4          # Number of responses per prompt (G in GRPO)\n",
    "    max_prompt_length: int = 256      # Maximum prompt token length\n",
    "    max_response_length: int = 512    # Maximum response token length\n",
    "    \n",
    "    # Training parameters\n",
    "    learning_rate: float = 1e-6\n",
    "    num_training_steps: int = 500\n",
    "    batch_size: int = 4\n",
    "    kl_coef: float = 0.1              # KL divergence coefficient\n",
    "    clip_range: float = 0.2           # PPO-style clipping\n",
    "    \n",
    "    # Generation parameters\n",
    "    temperature: float = 0.9\n",
    "    top_k: int = 40\n",
    "    top_p: float = 0.95\n",
    "    \n",
    "    # Reward weights\n",
    "    format_reward_weight: float = 0.3\n",
    "    correctness_reward_weight: float = 0.5\n",
    "    reasoning_quality_weight: float = 0.2\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir: str = \"/kaggle/working/checkpoints\"\n",
    "    save_every_n_steps: int = 100\n",
    "\n",
    "config = GRPOConfig()\n",
    "\n",
    "print(\"GRPO Configuration:\")\n",
    "print(f\"  Generations per prompt: {config.num_generations}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Training steps: {config.num_training_steps}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  KL coefficient: {config.kl_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reasoning training data\n",
    "# Expected format: [{\"question\": ..., \"answer\": ..., \"type\": ..., \"difficulty\": ...}, ...]\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/reasoning-training-data/reasoning_training_data.json\"\n",
    "\n",
    "# Try to load from Kaggle input, fall back to local\n",
    "try:\n",
    "    with open(DATA_PATH, 'r') as f:\n",
    "        training_data = json.load(f)\n",
    "    print(f\"Loaded {len(training_data)} examples from Kaggle input\")\n",
    "except FileNotFoundError:\n",
    "    # Fall back to local file\n",
    "    try:\n",
    "        with open('reasoning_training_data.json', 'r') as f:\n",
    "            training_data = json.load(f)\n",
    "        print(f\"Loaded {len(training_data)} examples from local file\")\n",
    "    except FileNotFoundError:\n",
    "        # Create sample data for demonstration\n",
    "        training_data = [\n",
    "            {\n",
    "                \"question\": \"What is 15% of 240?\",\n",
    "                \"answer\": \"36\",\n",
    "                \"type\": \"math\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"If a train travels at 60 mph for 2.5 hours, how far does it go?\",\n",
    "                \"answer\": \"150 miles\",\n",
    "                \"type\": \"math\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"A store offers 20% off. If an item costs $80, what is the sale price?\",\n",
    "                \"answer\": \"$64\",\n",
    "                \"type\": \"math\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            }\n",
    "        ]\n",
    "        print(f\"Using {len(training_data)} sample examples for demonstration\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample training example:\")\n",
    "print(f\"  Question: {training_data[0]['question']}\")\n",
    "print(f\"  Answer: {training_data[0]['answer']}\")\n",
    "print(f\"  Type: {training_data[0].get('type', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that shows your reasoning step by step.\n",
    "\n",
    "Instructions:\n",
    "1. Think through the problem carefully\n",
    "2. Show your reasoning in <reasoning> tags\n",
    "3. Put your final answer in <answer> tags\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "def format_prompt(question: str) -> str:\n",
    "    \"\"\"Format a question into the reasoning prompt template.\"\"\"\n",
    "    return REASONING_PROMPT_TEMPLATE.format(question=question)\n",
    "\n",
    "# Test prompt formatting\n",
    "test_prompt = format_prompt(training_data[0]['question'])\n",
    "print(\"Formatted prompt example:\")\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reasoning_and_answer(response: str) -> tuple:\n",
    "    \"\"\"Extract reasoning and answer from model response.\"\"\"\n",
    "    reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
    "    answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
    "    \n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "    answer = answer_match.group(1).strip() if answer_match else \"\"\n",
    "    \n",
    "    return reasoning, answer\n",
    "\n",
    "def format_reward(response: str) -> float:\n",
    "    \"\"\"Reward for correct output format.\"\"\"\n",
    "    has_reasoning = bool(re.search(r'<reasoning>.*?</reasoning>', response, re.DOTALL))\n",
    "    has_answer = bool(re.search(r'<answer>.*?</answer>', response, re.DOTALL))\n",
    "    \n",
    "    if has_reasoning and has_answer:\n",
    "        return 1.0\n",
    "    elif has_reasoning or has_answer:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def correctness_reward(response: str, expected_answer: str) -> float:\n",
    "    \"\"\"Reward for answer correctness.\"\"\"\n",
    "    _, extracted_answer = extract_reasoning_and_answer(response)\n",
    "    \n",
    "    if not extracted_answer:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize answers for comparison\n",
    "    extracted_normalized = extracted_answer.lower().strip()\n",
    "    expected_normalized = expected_answer.lower().strip()\n",
    "    \n",
    "    # Remove common punctuation and units for numerical comparison\n",
    "    extracted_clean = re.sub(r'[^0-9.-]', '', extracted_normalized)\n",
    "    expected_clean = re.sub(r'[^0-9.-]', '', expected_normalized)\n",
    "    \n",
    "    # Exact match\n",
    "    if extracted_normalized == expected_normalized:\n",
    "        return 1.0\n",
    "    \n",
    "    # Numerical match\n",
    "    try:\n",
    "        if extracted_clean and expected_clean:\n",
    "            if abs(float(extracted_clean) - float(expected_clean)) < 0.01:\n",
    "                return 1.0\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Partial match (answer contained in response)\n",
    "    if expected_normalized in extracted_normalized:\n",
    "        return 0.7\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "def reasoning_quality_reward(response: str) -> float:\n",
    "    \"\"\"Reward for reasoning quality.\"\"\"\n",
    "    reasoning, _ = extract_reasoning_and_answer(response)\n",
    "    \n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # Length check (not too short, not too long)\n",
    "    word_count = len(reasoning.split())\n",
    "    if 10 <= word_count <= 200:\n",
    "        score += 0.3\n",
    "    elif word_count > 5:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Contains step indicators\n",
    "    step_patterns = [r'first', r'then', r'next', r'finally', r'step', r'therefore', r'so', r'because']\n",
    "    step_matches = sum(1 for p in step_patterns if re.search(p, reasoning, re.IGNORECASE))\n",
    "    score += min(0.3, step_matches * 0.1)\n",
    "    \n",
    "    # Contains mathematical operations or logical connectors\n",
    "    math_patterns = [r'\\d+\\s*[+\\-*/]\\s*\\d+', r'=', r'multiply', r'divide', r'add', r'subtract']\n",
    "    math_matches = sum(1 for p in math_patterns if re.search(p, reasoning, re.IGNORECASE))\n",
    "    score += min(0.2, math_matches * 0.1)\n",
    "    \n",
    "    # Coherence: sentences end properly\n",
    "    sentences = reasoning.split('.')\n",
    "    if len(sentences) >= 2:\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(1.0, score)\n",
    "\n",
    "def compute_reward(response: str, expected_answer: str, config: GRPOConfig) -> float:\n",
    "    \"\"\"Compute total reward for a response.\"\"\"\n",
    "    fmt_reward = format_reward(response)\n",
    "    corr_reward = correctness_reward(response, expected_answer)\n",
    "    qual_reward = reasoning_quality_reward(response)\n",
    "    \n",
    "    total = (\n",
    "        config.format_reward_weight * fmt_reward +\n",
    "        config.correctness_reward_weight * corr_reward +\n",
    "        config.reasoning_quality_weight * qual_reward\n",
    "    )\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Test reward functions\n",
    "test_response = \"\"\"<reasoning>\n",
    "To find 15% of 240, I need to convert 15% to a decimal first.\n",
    "15% = 0.15\n",
    "Then multiply: 0.15 * 240 = 36\n",
    "</reasoning>\n",
    "<answer>36</answer>\"\"\"\n",
    "\n",
    "print(\"Reward function test:\")\n",
    "print(f\"  Format reward: {format_reward(test_response):.2f}\")\n",
    "print(f\"  Correctness reward: {correctness_reward(test_response, '36'):.2f}\")\n",
    "print(f\"  Reasoning quality reward: {reasoning_quality_reward(test_response):.2f}\")\n",
    "print(f\"  Total reward: {compute_reward(test_response, '36', config):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    \"\"\"GRPO Trainer for reasoning model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config: GRPOConfig, training_data: List[Dict]):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.training_data = training_data\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.metrics = {\n",
    "            'step': [],\n",
    "            'mean_reward': [],\n",
    "            'format_accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def sample_batch(self) -> List[Dict]:\n",
    "        \"\"\"Sample a batch of training examples.\"\"\"\n",
    "        indices = np.random.choice(len(self.training_data), self.config.batch_size, replace=False)\n",
    "        return [self.training_data[i] for i in indices]\n",
    "    \n",
    "    def generate_responses(self, prompt: str) -> List[str]:\n",
    "        \"\"\"Generate multiple responses for a prompt using the model.\"\"\"\n",
    "        responses = []\n",
    "        for _ in range(self.config.num_generations):\n",
    "            response = self.model.generate(\n",
    "                prompt,\n",
    "                max_length=self.config.max_response_length\n",
    "            )\n",
    "            # Extract only the generated part (after the prompt)\n",
    "            generated = response[len(prompt):] if response.startswith(prompt) else response\n",
    "            responses.append(generated)\n",
    "        return responses\n",
    "    \n",
    "    def compute_grpo_loss(self, responses: List[str], rewards: List[float]) -> float:\n",
    "        \"\"\"Compute GRPO loss based on relative rewards within the group.\"\"\"\n",
    "        rewards_array = np.array(rewards)\n",
    "        \n",
    "        # Compute advantages (relative to group mean)\n",
    "        mean_reward = np.mean(rewards_array)\n",
    "        std_reward = np.std(rewards_array) + 1e-8\n",
    "        advantages = (rewards_array - mean_reward) / std_reward\n",
    "        \n",
    "        # GRPO uses advantages directly without value function\n",
    "        # Higher advantage = better than group average = should be reinforced\n",
    "        loss = -np.mean(advantages * rewards_array)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, step: int) -> Dict[str, float]:\n",
    "        \"\"\"Execute one GRPO training step.\"\"\"\n",
    "        batch = self.sample_batch()\n",
    "        \n",
    "        all_rewards = []\n",
    "        format_correct = 0\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for example in batch:\n",
    "            prompt = format_prompt(example['question'])\n",
    "            expected_answer = example['answer']\n",
    "            \n",
    "            # Generate G responses\n",
    "            responses = self.generate_responses(prompt)\n",
    "            \n",
    "            # Compute rewards for each response\n",
    "            rewards = [compute_reward(r, expected_answer, self.config) for r in responses]\n",
    "            all_rewards.extend(rewards)\n",
    "            \n",
    "            # Track format accuracy\n",
    "            format_correct += sum(1 for r in responses if format_reward(r) == 1.0)\n",
    "            \n",
    "            # Compute GRPO loss\n",
    "            loss = self.compute_grpo_loss(responses, rewards)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Compute metrics\n",
    "        mean_reward = np.mean(all_rewards)\n",
    "        format_accuracy = format_correct / (len(batch) * self.config.num_generations)\n",
    "        avg_loss = total_loss / len(batch)\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': mean_reward,\n",
    "            'format_accuracy': format_accuracy,\n",
    "            'loss': avg_loss\n",
    "        }\n",
    "    \n",
    "    def train(self, num_steps: Optional[int] = None):\n",
    "        \"\"\"Run GRPO training loop.\"\"\"\n",
    "        num_steps = num_steps or self.config.num_training_steps\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"Starting GRPO Training\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training steps: {num_steps}\")\n",
    "        print(f\"Batch size: {self.config.batch_size}\")\n",
    "        print(f\"Generations per prompt: {self.config.num_generations}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        for step in tqdm(range(num_steps), desc=\"Training\"):\n",
    "            step_metrics = self.training_step(step)\n",
    "            \n",
    "            # Log metrics\n",
    "            self.metrics['step'].append(step)\n",
    "            self.metrics['mean_reward'].append(step_metrics['mean_reward'])\n",
    "            self.metrics['format_accuracy'].append(step_metrics['format_accuracy'])\n",
    "            self.metrics['loss'].append(step_metrics['loss'])\n",
    "            \n",
    "            # Print progress\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}: reward={step_metrics['mean_reward']:.3f}, \"\n",
    "                      f\"format_acc={step_metrics['format_accuracy']:.2%}, \"\n",
    "                      f\"loss={step_metrics['loss']:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if step > 0 and step % self.config.save_every_n_steps == 0:\n",
    "                self.save_checkpoint(step)\n",
    "        \n",
    "        # Final checkpoint\n",
    "        self.save_checkpoint(num_steps)\n",
    "        self.plot_metrics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Training Complete\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final mean reward: {np.mean(self.metrics['mean_reward'][-50:]):.3f}\")\n",
    "        print(f\"Final format accuracy: {np.mean(self.metrics['format_accuracy'][-50:]):.2%}\")\n",
    "    \n",
    "    def save_checkpoint(self, step: int):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.config.checkpoint_dir, f\"checkpoint_step_{step}\")\n",
    "        # Save metrics\n",
    "        metrics_path = os.path.join(self.config.checkpoint_dir, f\"metrics_step_{step}.json\")\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        print(f\"Saved checkpoint at step {step}\")\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Reward plot\n",
    "        axes[0].plot(self.metrics['step'], self.metrics['mean_reward'])\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Mean Reward')\n",
    "        axes[0].set_title('Training Reward')\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Format accuracy plot\n",
    "        axes[1].plot(self.metrics['step'], self.metrics['format_accuracy'])\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('Format Accuracy')\n",
    "        axes[1].set_title('Format Compliance')\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[2].plot(self.metrics['step'], self.metrics['loss'])\n",
    "        axes[2].set_xlabel('Step')\n",
    "        axes[2].set_ylabel('Loss')\n",
    "        axes[2].set_title('GRPO Loss')\n",
    "        axes[2].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.config.checkpoint_dir, 'training_curves.png'), dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"Saved training curves to {self.config.checkpoint_dir}/training_curves.png\")\n",
    "\n",
    "print(\"GRPOTrainer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Initialize and Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=gemma_lm,\n",
    "    config=config,\n",
    "    training_data=training_data\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"Training data size: {len(training_data)}\")\n",
    "print(f\"Checkpoint directory: {config.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "# For quick demo, use fewer steps\n",
    "DEMO_MODE = True\n",
    "num_steps = 50 if DEMO_MODE else config.num_training_steps\n",
    "\n",
    "print(f\"Running training for {num_steps} steps\")\n",
    "print(\"For full training, set DEMO_MODE = False\\n\")\n",
    "\n",
    "trainer.train(num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, question: str) -> str:\n",
    "    \"\"\"Run inference on a single question.\"\"\"\n",
    "    prompt = format_prompt(question)\n",
    "    response = model.generate(prompt, max_length=512)\n",
    "    \n",
    "    # Extract generated part\n",
    "    generated = response[len(prompt):] if response.startswith(prompt) else response\n",
    "    return generated\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is 25% of 180?\",\n",
    "    \"If a car travels at 55 mph for 3 hours, how far does it go?\",\n",
    "    \"A shirt costs $45 and is on sale for 30% off. What is the sale price?\"\n",
    "]\n",
    "\n",
    "print(\"Inference Demo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    response = inference(gemma_lm, question)\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    \n",
    "    # Evaluate response\n",
    "    reasoning, answer = extract_reasoning_and_answer(response)\n",
    "    print(f\"\\nExtracted:\")\n",
    "    print(f\"  Reasoning: {reasoning[:100]}...\" if len(reasoning) > 100 else f\"  Reasoning: {reasoning}\")\n",
    "    print(f\"  Answer: {answer}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Save Final Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "submission_dir = \"/kaggle/working/submission\"\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# Save final metrics\n",
    "final_metrics = {\n",
    "    'final_mean_reward': float(np.mean(trainer.metrics['mean_reward'][-50:])),\n",
    "    'final_format_accuracy': float(np.mean(trainer.metrics['format_accuracy'][-50:])),\n",
    "    'total_steps': len(trainer.metrics['step']),\n",
    "    'config': {\n",
    "        'num_generations': config.num_generations,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'batch_size': config.batch_size,\n",
    "        'kl_coef': config.kl_coef\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = os.path.join(submission_dir, 'final_metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(\"Final Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Reward: {final_metrics['final_mean_reward']:.3f}\")\n",
    "print(f\"Format Accuracy: {final_metrics['final_format_accuracy']:.2%}\")\n",
    "print(f\"Total Steps: {final_metrics['total_steps']}\")\n",
    "print(f\"\\nResults saved to: {submission_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Summary\n",
    "\n",
    "### What This Notebook Implements\n",
    "\n",
    "1. **GRPO Training**: Group Relative Policy Optimization using Tunix\n",
    "   - Generates multiple responses per prompt\n",
    "   - Computes rewards and advantages relative to group\n",
    "   - No value function required (unlike PPO)\n",
    "\n",
    "2. **Custom Reward Functions**:\n",
    "   - Format reward: Correct use of <reasoning> and <answer> tags\n",
    "   - Correctness reward: Answer matches expected value\n",
    "   - Reasoning quality: Step-by-step explanations\n",
    "\n",
    "3. **Output Format**: `<reasoning>...</reasoning><answer>...</answer>`\n",
    "\n",
    "### For Production Training\n",
    "\n",
    "1. Upload `reasoning_training_data.json` to Kaggle\n",
    "2. Set `DEMO_MODE = False` for full training\n",
    "3. Enable TPU v3-8 accelerator\n",
    "4. Run all cells sequentially\n",
    "\n",
    "### Model Checkpoint Location\n",
    "`/kaggle/working/checkpoints/`\n",
    "\n",
    "### Submission Artifacts\n",
    "`/kaggle/working/submission/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}